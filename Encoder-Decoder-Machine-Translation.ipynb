{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation as the name suggests is the process of conversion of data from one language to another with out the intervention of a human being. In other word it is the process by which a machine is taught how to convert text from one language to another. This is one of the most researched areas of Artificial Intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-anaconda\n",
    "-venv etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "-String\n",
    "-re\n",
    "-pickle\n",
    "-unicodedata\n",
    "-numpy\n",
    "-keras\n",
    "-pandas\n",
    "-pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pydotplus\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import string\n",
    "import re\n",
    "from pickle import dump,load\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set I have used for this particular consists of sentenses in English with their translations in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = '/home/codersarts/Desktop/data/deu.txt'\n",
    "destination = '/home/codersarts/Desktop/data/dg.pkl'\n",
    "destination_full  = '/home/codersarts/Desktop/data/dg_full.pkl'\n",
    "destination_train = '/home/codersarts/Desktop/data/dg_train.pkl'\n",
    "destination_test  = '/home/codersarts/Desktop/data/dg_test.pkl'\n",
    "file = open(loc, mode='rt', encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have is unstructured and needs to be structured for further processing. It consists of a lot of noise. In this step we remove the noise and the resultant would be key-value paires for sentences in english and german."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = text.strip().split('\\n')\n",
    "pairs = [line.split('\\t') for line in  lines]\n",
    "cleaned = list()\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "for pair in pairs:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            line = line.split()\n",
    "            line = [word.lower() for word in line]\n",
    "            line = [word.translate(table) for word in line]\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "cleaned = array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi --> hallo\n",
      "hi --> gru gott\n",
      "run --> lauf\n",
      "wow --> potzdonner\n",
      "wow --> donnerwetter\n",
      "fire --> feuer\n",
      "help --> hilfe\n",
      "help --> zu hulf\n",
      "stop --> stopp\n",
      "wait --> warte\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('%s --> %s' % (cleaned[i,0], cleaned[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump(cleaned, open(destination, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have clean data we need to divide this data into traing and test so that we can traing our model with training data and then further test in on test data respectively. I have taken 9000 instances of data as training samples and 1000 as test samples and then have saved them to the local memory as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = load(open(destination,'rb'))\n",
    "max_count = 10000\n",
    "\n",
    "dataset = raw_data[:max_count, :]\n",
    "shuffle(dataset)\n",
    "\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "\n",
    "dump(dataset, open(destination_full , 'wb'))\n",
    "dump(train, open(destination_train, 'wb'))\n",
    "dump(test, open(destination_test , 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load(open(destination_full, 'rb'))\n",
    "train = load(open(destination_train, 'rb'))\n",
    "test = load(open(destination_test, 'rb'))\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization & Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of spliting the data into tokens, word tokens or sent tokens. Seperate tokenizers are used for german and for english data. We can do this using tokenizer from keras functional api. Once the data data is tokenized then we need to turn it into sequences and pad these sequences. Again we do this using keras functional api. We have to do this for both training as well as test data. Also we need to encode our targets as categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2200\n",
      "English Max Length: 5\n"
     ]
    }
   ],
   "source": [
    "# English tokenizer\n",
    "tokenizerEN = Tokenizer()\n",
    "tokenizerEN.fit_on_texts(dataset[:,0])\n",
    "eng_vocab_size = len(tokenizerEN.word_index)+1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 3529\n",
      "German Max Length: 9\n"
     ]
    }
   ],
   "source": [
    "# German tokenizer\n",
    "tokenizerGE = Tokenizer()\n",
    "tokenizerGE.fit_on_texts(dataset[:,1])\n",
    "ger_vocab_size = len(tokenizerGE.word_index)+1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode/pad sequences for train\n",
    "x = tokenizerGE.texts_to_sequences(train[:,1])\n",
    "TrainX = pad_sequences(x , maxlen = ger_length , padding = 'post')\n",
    "\n",
    "y = tokenizerEN.texts_to_sequences(train[:,0])\n",
    "trainY = pad_sequences(y , maxlen = eng_length , padding = 'post')\n",
    "\n",
    "yl = list()\n",
    "for seq in trainY:\n",
    "    encod = to_categorical(seq , num_classes = eng_vocab_size)\n",
    "    yl.append(encod)\n",
    "var = array(yl)\n",
    "TrainY = var.reshape(trainY.shape[0], trainY.shape[1], eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode/pad sequences for test\n",
    "x = tokenizerGE.texts_to_sequences(test[:,1])\n",
    "TestX = pad_sequences(x , maxlen = ger_length , padding = 'post')\n",
    "\n",
    "y = tokenizerEN.texts_to_sequences(test[:,0])\n",
    "testY = pad_sequences(y , maxlen = eng_length , padding = 'post')\n",
    "\n",
    "yl = list()\n",
    "for seq in testY:\n",
    "    encod = to_categorical(seq , num_classes = eng_vocab_size)\n",
    "    yl.append(encod)\n",
    "var = array(yl)\n",
    "TestY = var.reshape(testY.shape[0], testY.shape[1], eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Design"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Neural networks have proven to be state of art technology for creation of language translation models. For this articals sake we have also implemented the same Encoder Decoder model in order to translate text form one language to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/29\n",
      "9000/9000 [==============================] - 273s 30ms/step - loss: 2.8242 - val_loss: 1.9814\n",
      "Epoch 2/29\n",
      "9000/9000 [==============================] - 209s 23ms/step - loss: 1.8990 - val_loss: 1.9146\n",
      "Epoch 3/29\n",
      "9000/9000 [==============================] - 193s 21ms/step - loss: 1.8056 - val_loss: 1.8194\n",
      "Epoch 4/29\n",
      "9000/9000 [==============================] - 164s 18ms/step - loss: 1.7435 - val_loss: 1.7961\n",
      "Epoch 5/29\n",
      "9000/9000 [==============================] - 164s 18ms/step - loss: 1.6888 - val_loss: 1.7375\n",
      "Epoch 6/29\n",
      "9000/9000 [==============================] - 139s 15ms/step - loss: 1.6246 - val_loss: 1.6963\n",
      "Epoch 7/29\n",
      "9000/9000 [==============================] - 124s 14ms/step - loss: 1.5604 - val_loss: 1.6496\n",
      "Epoch 8/29\n",
      "9000/9000 [==============================] - 112s 12ms/step - loss: 1.4966 - val_loss: 1.5614\n",
      "Epoch 9/29\n",
      "9000/9000 [==============================] - 114s 13ms/step - loss: 1.4030 - val_loss: 1.4959\n",
      "Epoch 10/29\n",
      "9000/9000 [==============================] - 90s 10ms/step - loss: 1.3190 - val_loss: 1.4428\n",
      "Epoch 11/29\n",
      "9000/9000 [==============================] - 80s 9ms/step - loss: 1.2401 - val_loss: 1.3875\n",
      "Epoch 12/29\n",
      "9000/9000 [==============================] - 97s 11ms/step - loss: 1.1678 - val_loss: 1.3279\n",
      "Epoch 13/29\n",
      "9000/9000 [==============================] - 80s 9ms/step - loss: 1.0977 - val_loss: 1.2931\n",
      "Epoch 14/29\n",
      "9000/9000 [==============================] - 88s 10ms/step - loss: 1.0321 - val_loss: 1.2610\n",
      "Epoch 15/29\n",
      "9000/9000 [==============================] - 84s 9ms/step - loss: 0.9701 - val_loss: 1.2266\n",
      "Epoch 16/29\n",
      "9000/9000 [==============================] - 76s 8ms/step - loss: 0.9098 - val_loss: 1.1874\n",
      "Epoch 17/29\n",
      "9000/9000 [==============================] - 82s 9ms/step - loss: 0.8548 - val_loss: 1.1706\n",
      "Epoch 18/29\n",
      "9000/9000 [==============================] - 88s 10ms/step - loss: 0.7992 - val_loss: 1.1428\n",
      "Epoch 19/29\n",
      "9000/9000 [==============================] - 89s 10ms/step - loss: 0.7448 - val_loss: 1.1164\n",
      "Epoch 20/29\n",
      "9000/9000 [==============================] - 81s 9ms/step - loss: 0.6935 - val_loss: 1.0929\n",
      "Epoch 21/29\n",
      "9000/9000 [==============================] - 92s 10ms/step - loss: 0.6415 - val_loss: 1.0770\n",
      "Epoch 22/29\n",
      "9000/9000 [==============================] - 134s 15ms/step - loss: 0.5933 - val_loss: 1.0572\n",
      "Epoch 23/29\n",
      "9000/9000 [==============================] - 185s 21ms/step - loss: 0.5490 - val_loss: 1.0393\n",
      "Epoch 24/29\n",
      "9000/9000 [==============================] - 173s 19ms/step - loss: 0.5034 - val_loss: 1.0244\n",
      "Epoch 25/29\n",
      "9000/9000 [==============================] - 196s 22ms/step - loss: 0.4611 - val_loss: 1.0120\n",
      "Epoch 26/29\n",
      "9000/9000 [==============================] - 168s 19ms/step - loss: 0.4222 - val_loss: 0.9960\n",
      "Epoch 27/29\n",
      "9000/9000 [==============================] - 158s 18ms/step - loss: 0.3845 - val_loss: 0.9855\n",
      "Epoch 28/29\n",
      "9000/9000 [==============================] - 106s 12ms/step - loss: 0.3490 - val_loss: 0.9786\n",
      "Epoch 29/29\n",
      "9000/9000 [==============================] - 98s 11ms/step - loss: 0.3181 - val_loss: 0.9753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf124bcdd0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(ger_vocab_size, 256, input_length = ger_length, mask_zero=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(RepeatVector(eng_length))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(eng_vocab_size, activation='softmax')))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "model.fit(TrainX, TrainY, epochs=29, batch_size=64, validation_data=(TestX, TestY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture for this model is shown below. The layers included are first the embedding layer for the extraction of latent features from the textual data. Then comes the encoder Long Short Term Memory layer. On top of LSTM is the repeat_vector to just repeat the input vector n times. Then comes another LSTM and finaly time distributed layer to wrap the dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 9, 256)            903424    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 9, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 9, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 9, 3529)           906953    \n",
      "=================================================================\n",
      "Total params: 2,861,001\n",
      "Trainable params: 2,861,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.vis_utils.pydot = pydot\n",
    "plot_model(model, to_file='/home/codersarts/Desktop/data/NTMmodel.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
